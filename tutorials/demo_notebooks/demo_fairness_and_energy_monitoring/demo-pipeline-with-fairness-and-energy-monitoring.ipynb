{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f976353c",
   "metadata": {},
   "source": [
    "# Demo KFP pipeline with fairness and energy monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a95f6d",
   "metadata": {},
   "source": [
    "This notebook demonstrates fairness and energy consumption monitoring in a single-run OSS pipeline. The used open-source GitHub repositories for enabling this monitoring are:  \n",
    "\n",
    "- Data: https://github.com/socialfoundations/folktables\n",
    "- Fairness: https://github.com/Trusted-AI/AIF360\n",
    "- Energy consumption: https://github.com/hubblo-org/scaphandre\n",
    "\n",
    "The used data is 2014 US Census PUMS data (https://www.census.gov/programs-surveys/acs/microdata.html) from California, which is preprocessed into an ACSIncome format by Folktables suite. This aims to replicate a prediction task standardized in ML fairness research by the widely used UCI Adult Dataset. The task is to make a model which predicts using the available features if an individual has an income higher than $50 000 (Give 0 for no and give 1 for yes) in the column PINCP. \n",
    "\n",
    "The sensitive attributes of this data are age (AGEP), gender (SEX), and ethnicity (RAC1P) with encodings found here: https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2014-2018.pdf. In this simple demonstration we will set the privileged group as white men under 50 ([{\"AGEP\":1, \"SEX\": 1, \"RAC1P\": 1}]), and the unprivileged group as non-white women over 50 ([{\"AGEP\":0, \"SEX\": 0, \"RAC1P\": 0}]), so the binarization thresholds are 50 for AGEP, 1 for SEX, and 1 for RAC1P. AIF enables setting up more distinct groups by adding more dict elements into privileged or unprivileged groups, like {'AGEP': 1}. For this reason, the resulted scores for these groups should be seen only as a technical demonstration and nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dashboard screenshot\n",
    "![Dashboard screenshot](dashboard-screenshot.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63ecfce2e341ad59"
  },
  {
   "cell_type": "markdown",
   "id": "8a7b7901",
   "metadata": {},
   "source": [
    "# Scaphandre, Prometheus alerts and Grafana dashboard setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d366c",
   "metadata": {},
   "source": [
    "Before we run the pipeline, we will need to set up Scaphander, Prometheus alerts and Grafana dashboard to handle metrics. \n",
    "\n",
    "To setup Scaphandre,we will use the following official documentation in the given order to get the required commands:\n",
    "\n",
    "- Kubernetes: https://hubblo-org.github.io/scaphandre-documentation/tutorials/kubernetes.html\n",
    "- Prometheus: https://hubblo-org.github.io/scaphandre-documentation/references/exporter-prometheus.html\n",
    "- Grafana: https://hubblo-org.github.io/scaphandre-documentation/how-to_guides/get-process-level-power-in-grafana.html\n",
    "\n",
    "We will first need to clone the Scaphandre GitHub Repository with the command 'git clone https://github.com/hubblo-org/scaphandre'. I cloned it into the folder where I stored the OSS clone, but it should be fine if it is anywhere else. When the cloning is done, call the command 'cd scaphandre', check that helm is installed with 'helm version', and then call 'helm install scaphandre helm/scaphandre'. \n",
    "\n",
    "However, it might be required that you change the default port value of 8080 from 8081 in 'scaphandre/helm/scaphandre/values.yaml' as seen in the provided 'modified-scaphandre-values.yaml' before doing the last step because it is the same port used by KFP, Kserve, and Prometheus. If you want to run Scaphandre with modified YAML, delete it with Helm using the command 'helm delete scaphandre' ('helm list' to confirm the name) and rerun the install command. \n",
    "\n",
    "You can check that Scahpandre configuration by running 'kubectl get pods' and then 'kubectl describe pod (pod name),' which should show under containers and Port that 8081/TCP. A faster way of doing this is 'kubectl get services.' To get Scahpandre to send metrics to Prometheus, we need to go into the pod with 'kubectl exec -it (pod name) -- /bin/bash' and then run 'scaphandre prometheus --port 8081'.\n",
    "\n",
    "If no errors are given, the setup is ready, and Prometheus should be able to query the energy consumption metrics. The Prometheus exporter can be made to run with 'scaphandre prometheus', but this will create errors due to the already used port. If the correct setup starts to throw errors unrelated to used port either due to nonoptimal configuration or unsuitable Prometheus query, restart it with CTRL + C and rerun the starting command.\n",
    "\n",
    "To setup Prometheus alerts for fairness metrics, we must modify the default 'prometheus-config-map.yaml' to have fairness alerts. It is recommended that the default YAML is first moved somewhere safe, after which the provided 'fairness-alert-prometheus-config-map.yaml' is renamed into 'prometheus-config-map.yaml'. Now we only need to run 'kubectl apply -k deployment/monitoring', 'kubectl rollout\n",
    "restart deployment/prometheus-deployment -n monitoring' and wait a bit to apply these modifications.\n",
    "\n",
    "To setup the Grafana dashboard for fairness and energy consumption metrics, we only need to click import under create and upload the provided JSON file named 'grafana_fairness_consumption_monitoring_1.json'. The dashboard will be empty, except for Prometheus alerts and consumption plots until the KFP pipeline has completed evaluation step. As long as Prometheus is capable of querying fairness and energy consumption metrics after running the KFP, Grafana should also be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88972adb",
   "metadata": {},
   "source": [
    "# KFP setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb90632",
   "metadata": {},
   "source": [
    "The requirements for running this code in Jupyter using a virtual enviroment are:\n",
    "\n",
    "- pip install notebook\n",
    "- pip install kfpâ‰ƒ1.8.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c53d8",
   "metadata": {},
   "source": [
    "Below we provide the necessary imports and set up the KFP client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fc367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    Artifact,\n",
    "    Model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Connect to client\n",
    "\n",
    "The default way of accessing Kubeflow is via port-forward. This enables you to get started quickly without imposing any requirements on your environment. Run the following to port-forward Istio's Ingress-Gateway to local port `8080`:\n",
    "\n",
    "```sh\n",
    "kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fe178106ef6769a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "def get_istio_auth_session(url: str, username: str, password: str) -> dict:\n",
    "    \"\"\"\n",
    "    Determine if the specified URL is secured by Dex and try to obtain a session cookie.\n",
    "    WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\n",
    "             (we default default to using `staticPasswords` if both are enabled)\n",
    "\n",
    "    :param url: Kubeflow server URL, including protocol\n",
    "    :param username: Dex `staticPasswords` or `LDAP` username\n",
    "    :param password: Dex `staticPasswords` or `LDAP` password\n",
    "    :return: auth session information\n",
    "    \"\"\"\n",
    "    # define the default return object\n",
    "    auth_session = {\n",
    "        \"endpoint_url\": url,    # KF endpoint URL\n",
    "        \"redirect_url\": None,   # KF redirect URL, if applicable\n",
    "        \"dex_login_url\": None,  # Dex login URL (for POST of credentials)\n",
    "        \"is_secured\": None,     # True if KF endpoint is secured\n",
    "        \"session_cookie\": None  # Resulting session cookies in the form \"key1=value1; key2=value2\"\n",
    "    }\n",
    "\n",
    "    # use a persistent session (for cookies)\n",
    "    with requests.Session() as s:\n",
    "\n",
    "        ################\n",
    "        # Determine if Endpoint is Secured\n",
    "        ################\n",
    "        resp = s.get(url, allow_redirects=True)\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"HTTP status code '{resp.status_code}' for GET against: {url}\"\n",
    "            )\n",
    "\n",
    "        auth_session[\"redirect_url\"] = resp.url\n",
    "\n",
    "        # if we were NOT redirected, then the endpoint is UNSECURED\n",
    "        if len(resp.history) == 0:\n",
    "            auth_session[\"is_secured\"] = False\n",
    "            return auth_session\n",
    "        else:\n",
    "            auth_session[\"is_secured\"] = True\n",
    "\n",
    "        ################\n",
    "        # Get Dex Login URL\n",
    "        ################\n",
    "        redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\n",
    "\n",
    "        # if we are at `/auth?=xxxx` path, we need to select an auth type\n",
    "        if re.search(r\"/auth$\", redirect_url_obj.path):\n",
    "\n",
    "            #######\n",
    "            # TIP: choose the default auth type by including ONE of the following\n",
    "            #######\n",
    "\n",
    "            # OPTION 1: set \"staticPasswords\" as default auth type\n",
    "            redirect_url_obj = redirect_url_obj._replace(\n",
    "                path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\n",
    "            )\n",
    "            # OPTION 2: set \"ldap\" as default auth type\n",
    "            # redirect_url_obj = redirect_url_obj._replace(\n",
    "            #     path=re.sub(r\"/auth$\", \"/auth/ldap\", redirect_url_obj.path)\n",
    "            # )\n",
    "\n",
    "        # if we are at `/auth/xxxx/login` path, then no further action is needed (we can use it for login POST)\n",
    "        if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\n",
    "            auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\n",
    "\n",
    "        # else, we need to be redirected to the actual login page\n",
    "        else:\n",
    "            # this GET should redirect us to the `/auth/xxxx/login` path\n",
    "            resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for GET against: {redirect_url_obj.geturl()}\"\n",
    "                )\n",
    "\n",
    "            # set the login url\n",
    "            auth_session[\"dex_login_url\"] = resp.url\n",
    "\n",
    "        ################\n",
    "        # Attempt Dex Login\n",
    "        ################\n",
    "        resp = s.post(\n",
    "            auth_session[\"dex_login_url\"],\n",
    "            data={\"login\": username, \"password\": password},\n",
    "            allow_redirects=True\n",
    "        )\n",
    "        if len(resp.history) == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Login credentials were probably invalid - \"\n",
    "                f\"No redirect after POST to: {auth_session['dex_login_url']}\"\n",
    "            )\n",
    "\n",
    "        # store the session cookies in a \"key1=value1; key2=value2\" string\n",
    "        auth_session[\"session_cookie\"] = \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
    "\n",
    "    return auth_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "KUBEFLOW_ENDPOINT = \"http://localhost:8080\"\n",
    "KUBEFLOW_USERNAME = \"user@example.com\"\n",
    "KUBEFLOW_PASSWORD = \"12341234\"\n",
    "\n",
    "auth_session = get_istio_auth_session(\n",
    "    url=KUBEFLOW_ENDPOINT,\n",
    "    username=KUBEFLOW_USERNAME,\n",
    "    password=KUBEFLOW_PASSWORD\n",
    ")\n",
    "\n",
    "client = kfp.Client(host=f\"{KUBEFLOW_ENDPOINT}/pipeline\", cookies=auth_session[\"session_cookie\"])\n",
    "# print(client.list_experiments())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee50e42302e51bd2"
  },
  {
   "cell_type": "markdown",
   "id": "a179fefb",
   "metadata": {},
   "source": [
    "# Pull data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879abbf8",
   "metadata": {},
   "source": [
    "Here we create a KFP component, which uses Folktables functions to get the data and preprocess it into a suitable format for the prediction task. This data is then made into an artifact for further usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcfcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.4.2\",\"numpy\",\"folktables\"],\n",
    "    output_component_file='components/pull_data_component.yaml',\n",
    ")\n",
    "def pull_data(\n",
    "    state: str, \n",
    "    year: int, \n",
    "    data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Pull data component.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    from folktables import ACSDataSource, ACSIncome\n",
    "    \n",
    "    pull_data_component_landmark = 'KFP_component'\n",
    "    \n",
    "    source = ACSDataSource(survey_year=year, horizon='1-Year', survey='person')\n",
    "    state_data = source.get_data(states=[state], download=True)\n",
    "    state_features, state_labels, _ = ACSIncome.df_to_pandas(state_data)\n",
    "    df = pd.concat([state_features,state_labels],axis=1)\n",
    "    df.to_csv(data.path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f5aa1",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21ea37",
   "metadata": {},
   "source": [
    "Here we create a component that changes sensitive attributes into binary columns using given thresholds, scales features (no sensitive attributes and predicted values), divides the data into three parts (train, test, indrift), and stores these parts as artifacts for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.4.2\", \"scikit-learn~=1.0.2\", \"numpy\"],\n",
    "    output_component_file='components/preprocess_component.yaml',\n",
    ")\n",
    "def preprocess(\n",
    "    data: Input[Dataset],\n",
    "    train_set: Output[Dataset],\n",
    "    test_set: Output[Dataset],\n",
    "    drift_set: Output[Dataset],\n",
    "    label_attribute: str,\n",
    "    sensitive_attributes: list,\n",
    "    splits: list,\n",
    "    group_thresholds: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess component.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from itertools import islice\n",
    "    \n",
    "    preprocess_component_landmark = 'KFP_component'\n",
    "    \n",
    "    l_a = label_attribute\n",
    "    s_a = sensitive_attributes\n",
    "    g_t = group_thresholds\n",
    "   \n",
    "    data = pd.read_csv(data.path)\n",
    "    \n",
    "    attribute_amount = len(s_a) + 1\n",
    "    non_scalable_attribute_identity = []\n",
    "    non_scalable_attribute_values = []\n",
    "\n",
    "    for i in range(0,attribute_amount):\n",
    "        if i == attribute_amount-1:\n",
    "            non_scalable_attribute_identity.append(l_a)\n",
    "            non_scalable_attribute_values.append(data[l_a].astype(int))\n",
    "            del data[l_a]\n",
    "            continue\n",
    "\n",
    "        values = data[s_a[i]].copy()\n",
    "        values[values <= g_t[i]] = 1\n",
    "        values[values > g_t[i]] = 0\n",
    "        non_scalable_attribute_identity.append(s_a[i])\n",
    "        non_scalable_attribute_values.append(values)\n",
    "        del data[s_a[i]]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    bin_df = pd.DataFrame(scaler.fit_transform(data), \n",
    "                                 columns=data.columns) \n",
    "    \n",
    "    index = 0\n",
    "    for name in non_scalable_attribute_identity:\n",
    "        bin_df[name] = np.array(non_scalable_attribute_values[index]).astype(int)\n",
    "        index = index + 1\n",
    "\n",
    "    index = np.array(bin_df.index)\n",
    "    random.seed(42)\n",
    "    random.shuffle(index)\n",
    "\n",
    "    amounts = [round(bin_df.shape[0]*splits[0]), \n",
    "               round(bin_df.shape[0]*splits[1]), \n",
    "               round(bin_df.shape[0]*splits[2])]\n",
    "\n",
    "    if bin_df.shape[0] < sum(amounts):\n",
    "            amounts[4] = amounts[4]+(bin_df.shape[0]-sum(amounts)) \n",
    "\n",
    "    it = iter(index)\n",
    "\n",
    "    sliced = [list(islice(it, 0, i)) for i in amounts]\n",
    "\n",
    "    train_data = bin_df.loc[sliced[0]]\n",
    "    test_data = bin_df.loc[sliced[1]]\n",
    "    drift_data = bin_df.loc[sliced[2]]\n",
    "\n",
    "    train_data.to_csv(train_set.path, index=None)\n",
    "    test_data.to_csv(test_set.path, index=None)\n",
    "    drift_data.to_csv(drift_set.path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf0229",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651da6a2",
   "metadata": {},
   "source": [
    "Here we create a component that defines the used metrics, makes the datasets suitable for AIF360 metrics (with a favorable label of 1 and unfavorable label of 0), trains a logistic regression model, calculates metrics, and stores these metrics into MLflow for model comparison. Some of the code is reused from the original OSS pipeline. The used fairness metrics are statistical parity, disparate impact, equal odds difference, average odds difference, and theil index fairness metrics, which are the most common in the provided AIF360 tutorials. Notice that there are fewer dataset metrics than model metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b400201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"numpy\", \n",
    "                         \"pandas~=1.4.2\",\n",
    "                         \"aif360\",\n",
    "                         \"scikit-learn~=1.0.2\", \n",
    "                         \"mlflow~=2.4.1\", \n",
    "                         \"boto3~=1.21.0\"],\n",
    "    output_component_file='components/train_component.yaml',\n",
    ")\n",
    "def train(\n",
    "    train_set: Input[Dataset],\n",
    "    test_set: Input[Dataset],\n",
    "    saved_model: Output[Model],\n",
    "    model_name: str,\n",
    "    label_attribute: str,\n",
    "    sensitive_attributes: list,\n",
    "    privilaged_groups: list,\n",
    "    unprivilaged_groups: list,\n",
    "    mlflow_experiment_name: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    mlflow_s3_endpoint_url: str\n",
    ") -> NamedTuple(\"Output\", [('storage_uri', str), ('run_id', str),]):\n",
    "    \"\"\"\n",
    "    Train component.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    from aif360.datasets import BinaryLabelDataset\n",
    "    from aif360.metrics import BinaryLabelDatasetMetric\n",
    "    from aif360.metrics import ClassificationMetric\n",
    "    from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n",
    "    \n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    import os\n",
    "    import logging\n",
    "    import pickle\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    train_component_landmark = 'KFP_component'\n",
    "    \n",
    "    l_a = label_attribute\n",
    "    s_a = sensitive_attributes\n",
    "    p_g = privilaged_groups\n",
    "    u_g = unprivilaged_groups\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def dataset_fairness(dataset,p_g,u_g):\n",
    "        metrics_list = []\n",
    "        dataset_metrics = BinaryLabelDatasetMetric(dataset, \n",
    "                                       privileged_groups = p_g,\n",
    "                                       unprivileged_groups = u_g)\n",
    "        \n",
    "        SP = dataset_metrics.mean_difference()\n",
    "        DI = dataset_metrics.disparate_impact()\n",
    "        \n",
    "        # Statistical parity\n",
    "        metrics_list.append({'name': 'D_SP', \n",
    "                             'value': SP })\n",
    "        \n",
    "        # Disparate impact\n",
    "        metrics_list.append({'name': 'D_DI', \n",
    "                             'value': DI })\n",
    "        \n",
    "        return metrics_list\n",
    "    \n",
    "    def model_metrics(dataset, pred, p_g, u_g):\n",
    "        metrics_list = []\n",
    "        \n",
    "        Acc = accuracy_score(dataset.labels, pred)\n",
    "        \n",
    "        # Accuracy\n",
    "        metrics_list.append({'name': 'M_Acc', \n",
    "                             'value': Acc })\n",
    "        \n",
    "        matrix = confusion_matrix(dataset.labels, pred)\n",
    "        \n",
    "        # True positives\n",
    "        metrics_list.append({'name': 'M_TP', \n",
    "                             'value': matrix[0][0]})\n",
    "        # False positives\n",
    "        metrics_list.append({'name': 'M_FP', \n",
    "                             'value': matrix[0][1]})\n",
    "        # False negatives\n",
    "        metrics_list.append({'name': 'M_FN', \n",
    "                             'value': matrix[1][0]})\n",
    "        # True negatives\n",
    "        metrics_list.append({'name': 'M_TN', \n",
    "                             'value': matrix[1][1]})\n",
    "        \n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "        \n",
    "        model_metrics = ClassificationMetric(\n",
    "                        dataset,\n",
    "                        dataset_pred,\n",
    "                        privileged_groups = p_g,\n",
    "                        unprivileged_groups = u_g)\n",
    "        \n",
    "        BA = (model_metrics.true_positive_rate() + model_metrics.true_negative_rate()) / 2\n",
    "         \n",
    "        # Balanced accuracy\n",
    "        metrics_list.append({'name': 'M_BA', \n",
    "                             'value': BA})   \n",
    "            \n",
    "        SP = model_metrics.mean_difference()\n",
    "        DI = model_metrics.disparate_impact()\n",
    "        AOD = model_metrics.average_odds_difference()\n",
    "        EOD = model_metrics.equal_opportunity_difference()\n",
    "        TI = model_metrics.theil_index()\n",
    "        \n",
    "        # Statistical parity\n",
    "        metrics_list.append({'name': 'M_SP', \n",
    "                             'value': SP})\n",
    "        # Disparate impact\n",
    "        metrics_list.append({'name': 'M_DI', \n",
    "                             'value': DI})\n",
    "        \n",
    "        # Average odds difference\n",
    "        metrics_list.append({'name': 'M_AOD', \n",
    "                             'value': AOD})\n",
    "        \n",
    "        # Equal oppoturnity difference\n",
    "        metrics_list.append({'name': 'M_EOD', \n",
    "                             'value': EOD})\n",
    "        \n",
    "        # Theil index\n",
    "        metrics_list.append({'name': 'M_TI', \n",
    "                             'value': TI})\n",
    "        \n",
    "        return metrics_list\n",
    "    \n",
    "    os.environ['MLFLOW_S3_ENDPOINT_URL'] = mlflow_s3_endpoint_url\n",
    "\n",
    "    # load data\n",
    "    logger.info(\"Setting up data\")\n",
    "    train_data = pd.read_csv(train_set.path)\n",
    "    test_data = pd.read_csv(test_set.path)\n",
    "    \n",
    "    train = BinaryLabelDataset(\n",
    "                   favorable_label = 1,\n",
    "                   unfavorable_label = 0,\n",
    "                   df = train_data,\n",
    "                   label_names = [l_a],\n",
    "                   protected_attribute_names = s_a)\n",
    "\n",
    "    test = BinaryLabelDataset(\n",
    "               favorable_label = 1,\n",
    "               unfavorable_label = 0,\n",
    "               df = test_data,\n",
    "               label_names = [l_a],\n",
    "               protected_attribute_names= s_a)\n",
    "    \n",
    "    logger.info(\"Checking training and test data fairness\")\n",
    "    train_fairness = dataset_fairness(train,p_g,u_g)\n",
    "    test_fairness = dataset_fairness(test,p_g,u_g)\n",
    "\n",
    "    # The predicted column is \"Target\" which is either 0 or 1\n",
    "    train_x = train.features \n",
    "    test_x = test.features \n",
    "    train_y = train.labels \n",
    "    test_y = test.labels \n",
    "    \n",
    "    logger.info(f\"Using MLflow tracking URI: {mlflow_tracking_uri}\")\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "    logger.info(f\"Using MLflow experiment: {mlflow_experiment_name}\")\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Run ID: {run_id}\")\n",
    "\n",
    "        model = LogisticRegression(random_state=42)\n",
    "        \n",
    "        logger.info(\"Fitting model...\")\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        logger.info(\"Predicting...\")\n",
    "        \n",
    "        predicted_qualities = model.predict(test_x)\n",
    "        \n",
    "        model_metrics = model_metrics(test, predicted_qualities, p_g, u_g)\n",
    "        \n",
    "        logger.info(\"Logging training data metrics to MLflow\")\n",
    "        for pair in train_fairness:\n",
    "            name = 'Tr_' + pair['name']\n",
    "            mlflow.log_metric(name, pair['value'])\n",
    "        \n",
    "        logger.info(\"Logging test data metrics to MLflow\")\n",
    "        for pair in test_fairness:\n",
    "            name = 'Te_' + pair['name']\n",
    "            mlflow.log_metric(name, pair['value'])\n",
    "        \n",
    "        logger.info(\"Logging model metrics to MLflow\")\n",
    "        for pair in model_metrics:\n",
    "            mlflow.log_metric(pair['name'], pair['value'])\n",
    "        \n",
    "        # save model to mlflow\n",
    "        logger.info(\"Logging trained model\")\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            model_name,\n",
    "            registered_model_name=\"USCensusLR\",\n",
    "            serialization_format=\"pickle\"\n",
    "        )\n",
    "\n",
    "        logger.info(\"Logging predictions artifact to MLflow\")\n",
    "        np.save(\"predictions.npy\", predicted_qualities)\n",
    "        mlflow.log_artifact(\n",
    "            local_path=\"predictions.npy\", artifact_path=\"predicted_qualities/\"\n",
    "        )\n",
    "\n",
    "        # save model as KFP artifact\n",
    "        logging.info(f\"Saving model to: {saved_model.path}\")\n",
    "        with open(saved_model.path, 'wb') as fp:\n",
    "            pickle.dump(model, fp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # prepare output\n",
    "        output = namedtuple('Output', ['storage_uri', 'run_id'])\n",
    "\n",
    "        # return str(mlflow.get_artifact_uri())\n",
    "        return output(mlflow.get_artifact_uri(), run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd87b60",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77dad7",
   "metadata": {},
   "source": [
    "Here we define a component, which gets the stored metrics, pushes these into a Prometheus gateway, and evaluates these metrics with given thresholds before going to the next phase. The Prometheus gateway is a ready-made component of the OSS pipeline, which Prometheus will scrape by providing the correct gateway URL. Prometheus is set up with a slightly modified YAML configuration to alert when accuracy and fairness metrics exceed given thresholds. Prometheus also enables Grafana to easily visualize the given metrics to provide a general overview of the deployed model and the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"numpy\", \"mlflow~=1.25.0\", \"prometheus_client\"],\n",
    "    output_component_file='components/evaluate_component.yaml',\n",
    ")\n",
    "def evaluate(\n",
    "    run_id: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    threshold_metrics: dict\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Evaluate component: Compares metrics from training with given thresholds.\n",
    "\n",
    "    Args:\n",
    "        run_id (string):  MLflow run ID\n",
    "        mlflow_tracking_uri (string): MLflow tracking URI\n",
    "        threshold_metrics (dict): Minimum threshold values for each metric\n",
    "    Returns:\n",
    "        Bool indicating whether evaluation passed or failed.\n",
    "    \"\"\"\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "    import requests\n",
    "    import json\n",
    "    import logging\n",
    "    \n",
    "    evaluate_component_landmark = 'KFP_component'\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    client = MlflowClient(tracking_uri=mlflow_tracking_uri)\n",
    "    info = client.get_run(run_id)\n",
    "    training_metrics = info.data.metrics\n",
    "\n",
    "    logger.info(f\"Training metrics: {training_metrics}\")\n",
    "    \n",
    "    registry = CollectorRegistry()\n",
    "    url = 'http://prometheus-pushgateway.monitoring.svc.cluster.local:9091'\n",
    "    for key, value in training_metrics.items():\n",
    "        metric = Gauge(key, 'Metric', registry = registry)\n",
    "        metric.set(value)\n",
    "    push_to_gateway(url, job = 'Metrics', registry = registry)\n",
    "    \n",
    "    # compare the evaluation metrics with the defined thresholds\n",
    "    for key, value in threshold_metrics.items():\n",
    "        if (key not in training_metrics) or (training_metrics[key] < value):\n",
    "            logger.error(f\"Metric {key} failed. Evaluation not passed!\")\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a77e2",
   "metadata": {},
   "source": [
    "# Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5cdfc",
   "metadata": {},
   "source": [
    "Here we define a component that deploys the passed model into an inference service. This component is exactly similar to the original OSS demo pipeline deploy model component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"kserve==0.11.0\"],\n",
    "    output_component_file='components/deploy_model_component.yaml',\n",
    ")\n",
    "def deploy_model(model_name: str, storage_uri: str):\n",
    "    \"\"\"\n",
    "    Deploy the model as a inference service with Kserve.\n",
    "    \"\"\"\n",
    "    from kubernetes import client\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    import logging\n",
    "    \n",
    "    deploy_model_component_landmark = 'KFP_component'\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    model_uri = f\"{storage_uri}/{model_name}\"\n",
    "    logger.info(\"MODEL URI:\", model_uri)\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=model_name,\n",
    "            namespace=namespace,\n",
    "            annotations={'sidecar.istio.io/inject':'false'}\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                service_account_name=\"kserve-sa\",\n",
    "                sklearn=V1beta1SKLearnSpec(\n",
    "                    storage_uri=model_uri\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb274bf",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cad7e",
   "metadata": {},
   "source": [
    "Here we define a component that tests out the deployed inference service. The only difference between this and the original OSS pipeline component is that this gives two already preprocessed samples for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",  # kserve on python 3.10 comes with a dependency that fails to get installed\n",
    "    packages_to_install=[\"kserve==0.11.0\", \"scikit-learn~=1.0.2\"],\n",
    "    output_component_file='components/inference_component.yaml',\n",
    ")\n",
    "def inference(\n",
    "    model_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Test inference.\n",
    "    \"\"\"\n",
    "    from kserve import KServeClient\n",
    "    from kserve import utils\n",
    "    import requests\n",
    "    import pickle\n",
    "    import logging\n",
    "    from urllib.parse import urlsplit\n",
    "    import re\n",
    "    \n",
    "    inference_component_landmark = 'KFP_component'\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def get_istio_auth_session(url: str, username: str, password: str) -> dict:\n",
    "        \"\"\"\n",
    "        Determine if the specified URL is secured by Dex and try to obtain a session cookie.\n",
    "        WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\n",
    "                 (we default default to using `staticPasswords` if both are enabled)\n",
    "    \n",
    "        :param url: Kubeflow server URL, including protocol\n",
    "        :param username: Dex `staticPasswords` or `LDAP` username\n",
    "        :param password: Dex `staticPasswords` or `LDAP` password\n",
    "        :return: auth session information\n",
    "        \"\"\"\n",
    "        # define the default return object\n",
    "        auth_session = {\n",
    "            \"endpoint_url\": url,    # KF endpoint URL\n",
    "            \"redirect_url\": None,   # KF redirect URL, if applicable\n",
    "            \"dex_login_url\": None,  # Dex login URL (for POST of credentials)\n",
    "            \"is_secured\": None,     # True if KF endpoint is secured\n",
    "            \"session_cookie\": None  # Resulting session cookies in the form \"key1=value1; key2=value2\"\n",
    "        }\n",
    "    \n",
    "        # use a persistent session (for cookies)\n",
    "        with requests.Session() as s:\n",
    "    \n",
    "            ################\n",
    "            # Determine if Endpoint is Secured\n",
    "            ################\n",
    "            resp = s.get(url, allow_redirects=True)\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for GET against: {url}\"\n",
    "                )\n",
    "    \n",
    "            auth_session[\"redirect_url\"] = resp.url\n",
    "    \n",
    "            # if we were NOT redirected, then the endpoint is UNSECURED\n",
    "            if len(resp.history) == 0:\n",
    "                auth_session[\"is_secured\"] = False\n",
    "                return auth_session\n",
    "            else:\n",
    "                auth_session[\"is_secured\"] = True\n",
    "    \n",
    "            ################\n",
    "            # Get Dex Login URL\n",
    "            ################\n",
    "            redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\n",
    "    \n",
    "            # if we are at `/auth?=xxxx` path, we need to select an auth type\n",
    "            if re.search(r\"/auth$\", redirect_url_obj.path):\n",
    "    \n",
    "                #######\n",
    "                # TIP: choose the default auth type by including ONE of the following\n",
    "                #######\n",
    "    \n",
    "                # OPTION 1: set \"staticPasswords\" as default auth type\n",
    "                redirect_url_obj = redirect_url_obj._replace(\n",
    "                    path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\n",
    "                )\n",
    "                # OPTION 2: set \"ldap\" as default auth type\n",
    "                # redirect_url_obj = redirect_url_obj._replace(\n",
    "                #     path=re.sub(r\"/auth$\", \"/auth/ldap\", redirect_url_obj.path)\n",
    "                # )\n",
    "    \n",
    "            # if we are at `/auth/xxxx/login` path, then no further action is needed (we can use it for login POST)\n",
    "            if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\n",
    "                auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\n",
    "    \n",
    "            # else, we need to be redirected to the actual login page\n",
    "            else:\n",
    "                # this GET should redirect us to the `/auth/xxxx/login` path\n",
    "                resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\n",
    "                if resp.status_code != 200:\n",
    "                    raise RuntimeError(\n",
    "                        f\"HTTP status code '{resp.status_code}' for GET against: {redirect_url_obj.geturl()}\"\n",
    "                    )\n",
    "    \n",
    "                # set the login url\n",
    "                auth_session[\"dex_login_url\"] = resp.url\n",
    "    \n",
    "            ################\n",
    "            # Attempt Dex Login\n",
    "            ################\n",
    "            resp = s.post(\n",
    "                auth_session[\"dex_login_url\"],\n",
    "                data={\"login\": username, \"password\": password},\n",
    "                allow_redirects=True\n",
    "            )\n",
    "            if len(resp.history) == 0:\n",
    "                raise RuntimeError(\n",
    "                    f\"Login credentials were probably invalid - \"\n",
    "                    f\"No redirect after POST to: {auth_session['dex_login_url']}\"\n",
    "                )\n",
    "    \n",
    "            # store the session cookies in a \"key1=value1; key2=value2\" string\n",
    "            auth_session[\"session_cookie\"] = \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
    "    \n",
    "        return auth_session\n",
    "    \n",
    "    KUBEFLOW_ENDPOINT = \"http://istio-ingressgateway.istio-system.svc.cluster.local:80\"\n",
    "    KUBEFLOW_USERNAME = \"user@example.com\"\n",
    "    KUBEFLOW_PASSWORD = \"12341234\"\n",
    "    \n",
    "    auth_session = get_istio_auth_session(\n",
    "    url=KUBEFLOW_ENDPOINT,\n",
    "    username=KUBEFLOW_USERNAME,\n",
    "    password=KUBEFLOW_PASSWORD,\n",
    "    )\n",
    "    TOKEN = auth_session[\"session_cookie\"].replace(\"authservice_session=\", \"\")\n",
    "    print(\"Token:\", TOKEN)\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    \n",
    "    input_sample = [\n",
    "        [-0.07237661,  0.92825499,  1.31739921, -1.31196944, -0.73834281,\n",
    "         -0.09041844,  0.18720611,  1.        ,  0.        ,  1.        ],\n",
    "        [-0.60712221, -1.55312352,  1.31739921,  0.05298942,  1.63138481,\n",
    "         -0.54927344,  0.18720611,  1.        ,  0.        ,  1.        ]]\n",
    "    \n",
    "    # get inference service\n",
    "    KServe = KServeClient()\n",
    "\n",
    "    # wait for deployment to be ready\n",
    "    KServe.get(model_name, namespace=namespace, watch=True, timeout_seconds=120)\n",
    "\n",
    "    inference_service = KServe.get(model_name, namespace=namespace)\n",
    "    is_url = f\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/v1/models/{model_name}:predict\"\n",
    "    header = {\"Host\": f\"{model_name}.{namespace}.example.com\"}\n",
    "\n",
    "    logger.info(f\"\\nInference service status:\\n{inference_service['status']}\")\n",
    "    logger.info(f\"\\nInference service URL:\\n{is_url}\\n\")\n",
    "    \n",
    "    inference_input = {\n",
    "        'instances': input_sample\n",
    "    }\n",
    "    response = requests.post(\n",
    "        is_url,\n",
    "        json=inference_input,\n",
    "        headers=header,\n",
    "        cookies={\"authservice_session\": TOKEN}\n",
    "        \n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP status code '{response.status_code}': {response.json()}\")\n",
    "    logger.info(f\"\\nPrediction response:\\n{response.text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb4b46",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abfd0f",
   "metadata": {},
   "source": [
    "The code below uses the previously defined components to create a KFP pipeline. This code modifies the original OSS pipeline by giving new variables state, year, lable_attribute, sensitive_attributes, splits, privilaged_groups, unprivilaged_groups, and group_thresholds for pull, preprocess, and train phases to use the new code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "      name='aif-pipeline',\n",
    "      description='An single run pipeline for UCI Adult prediciton task',\n",
    ")\n",
    "def pipeline(\n",
    "    state: str,\n",
    "    year: int,\n",
    "    label_attribute: str,\n",
    "    sensitive_attributes: list,\n",
    "    splits: list,\n",
    "    privilaged_groups: list,\n",
    "    unprivilaged_groups: list,\n",
    "    group_thresholds: list,\n",
    "    mlflow_experiment_name: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    mlflow_s3_endpoint_url: str,\n",
    "    model_name: str,\n",
    "    threshold_metrics: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    pipeline component.\n",
    "    \"\"\"\n",
    "    pipeline_landmark = 'KFP_pipeline'\n",
    "    \n",
    "    pull_task = pull_data(state = state, year = year)\n",
    "\n",
    "    preprocess_task = preprocess(data=pull_task.outputs[\"data\"],\n",
    "                                 label_attribute = label_attribute,\n",
    "                                 sensitive_attributes = sensitive_attributes,\n",
    "                                 splits = splits, group_thresholds = group_thresholds)\n",
    "\n",
    "    train_task = train(\n",
    "        train_set = preprocess_task.outputs[\"train_set\"],\n",
    "        test_set = preprocess_task.outputs[\"test_set\"],\n",
    "        mlflow_experiment_name=mlflow_experiment_name,\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        mlflow_s3_endpoint_url=mlflow_s3_endpoint_url,\n",
    "        model_name=model_name,\n",
    "        label_attribute = label_attribute,\n",
    "        sensitive_attributes = sensitive_attributes,\n",
    "        privilaged_groups = privilaged_groups,\n",
    "        unprivilaged_groups = unprivilaged_groups\n",
    "    )\n",
    "    \n",
    "    train_task.apply(use_aws_secret(secret_name=\"aws-secret\"))\n",
    "\n",
    "    evaluate_trask = evaluate(\n",
    "        run_id=train_task.outputs[\"run_id\"],\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        threshold_metrics=threshold_metrics\n",
    "    )\n",
    "    \n",
    "    eval_passed = evaluate_trask.output\n",
    "\n",
    "    with dsl.Condition(eval_passed == \"true\"):\n",
    "        deploy_model_task = deploy_model(\n",
    "            model_name=model_name,\n",
    "            storage_uri=train_task.outputs[\"storage_uri\"],\n",
    "        )\n",
    "\n",
    "        inference_task = inference(\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        inference_task.after(deploy_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab72374",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fecf692",
   "metadata": {},
   "source": [
    "Here we define the arguments used by the pipeline. Notice how the data is split with a 0.5-0.3-0.2 ratio into train, test, and indrift datasets. We also see how AIF360 defines compared groups in privilaged_groups and unprivilaged_groups values. We can change the state and year if we want this pipeline to use other PUMS data. Just pick any state from seen here https://www.bls.gov/respondents/mwr/electronic-data-interchange/appendix-d-usps-state-abbreviations-and-fips-codes.htm and choose between a year in the range [2014,2018]. \n",
    "\n",
    "However, the memory requirements will increase since folktables will download the data, and then KFP needs to make a new artifact. It might also be sometimes the case that KFP will create separate artifacts for the same data, but this has not happened in this pipeline setup. If you want to check created artifacts, go to the KFP dashboard, and click artifacts and unknown.\n",
    "\n",
    "If the pipeline, as seen in the KFP dashboard, gets an error in the inference step, you must either change the model_name given in the arguments into something else or remove some of the existing inference services. The latter can be done by writing 'kubectl get isvc -n kserve-inference' and using the listed names in the 'kubectl -n kserve-inference delete isvc (model_name)'. You can check these and other existing services with 'Kubectl get services -A'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want the pipeline obey certain thresholds, we can set them here\n",
    "eval_threshold_metrics = {'M_Acc': 0.60}\n",
    "\n",
    "arguments = {\n",
    "    \"state\": \"CA\",\n",
    "    \"year\": 2014,\n",
    "    \"label_attribute\": \"PINCP\",\n",
    "    \"sensitive_attributes\": [\"AGEP\",\"SEX\",\"RAC1P\"],\n",
    "    \"splits\": [0.5,0.3,0.2],\n",
    "    \"group_thresholds\": [50,1,1],\n",
    "    \"privilaged_groups\": [{\"AGEP\":1, \"SEX\": 1, \"RAC1P\": 1}],\n",
    "    \"unprivilaged_groups\": [{\"AGEP\":0, \"SEX\": 0, \"RAC1P\": 0}],\n",
    "    \"mlflow_tracking_uri\": \"http://mlflow.mlflow.svc.cluster.local:5000\",\n",
    "    \"mlflow_s3_endpoint_url\": \"http://mlflow-minio-service.mlflow.svc.cluster.local:9000\",\n",
    "    \"mlflow_experiment_name\": \"demo-aif-notebook\",\n",
    "    \"model_name\": \"demo-aif-lr\",\n",
    "    \"threshold_metrics\": eval_threshold_metrics\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faba84c",
   "metadata": {},
   "source": [
    "# Submit run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd08ad",
   "metadata": {},
   "source": [
    "This block enables running the constructed pipeline. If you want to update the pipeline, rerun the code you have changed and then rerun this block. For some reason, running this KFP pipeline creates temporary mystery files with a size range of [2,5] GB, which enable_caching, metadata writing, artifacts, or in optimal docker configuration might cause. On their own, these files are not a problem, but if a computer has a set memory space of 105 GB for the root file system that has around 20GB free space left due to it being the default place for all kinds of software, the user needs to either manually delete these files (possible places for these files in ubuntu 22.04 are /tmp and /run based on date modification) or restart the computer after running KFP around 4-5 times to continue rerunning the pipeline. Thus, checking your computer's memory before and after running the KFP pipeline is recommended to prevent possible mistakes. It might also be worthwhile to check the amount of memory Minio is using with the following port forwards:\n",
    "\n",
    "- MLFlow Minio = kubectl -n mlflow port-forward svc/mlflow-minio-service 9001:9001 (user and password is minioadmin)\n",
    "- KFP Minio = kubectl port-forward -n kubeflow svc/minio-service 9000:9000 (user is minio and password minio123)\n",
    "\n",
    "The localhost URLs are:\n",
    "\n",
    "- MLFlow Minio = http://localhost:9001/\n",
    "- KFP Minio = http://localhost:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"demo-aif-run\"\n",
    "experiment_name = \"demo-aif-experiment\"\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=pipeline,\n",
    "    run_name=run_name,\n",
    "    experiment_name=experiment_name,\n",
    "    arguments=arguments,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
    "    enable_caching=False,\n",
    "    namespace=\"kubeflow-user-example-com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb432f9",
   "metadata": {},
   "source": [
    "# Demonstration confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55974312",
   "metadata": {},
   "source": [
    "If the previous block did not create any errors, check how the run goes by port forwarding KFP, MLFlow, Pushgateway, Prometheus, and Grafana. Since KFP and Prometheus ports are the same, I recommend first waiting for the pipeline to run in the KFP dashboard, shutting it down, and then port forwarding Prometheus. Same for Grafana, since it uses the same port as MLFlow. Here are the commands required for these port forwards:\n",
    "\n",
    "- KFP = kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n",
    "- MLFlow = kubectl -n mlflow port-forward svc/mlflow 5000:5000\n",
    "- Pushgateway = kubectl port-forward svc/prometheus-pushgateway 9091 -n monitoring\n",
    "- Promtheus = kubectl port-forward svc/prometheus-service 8080 -n monitoring\n",
    "- Grafana = kubectl port-forward svc/grafana 5000:3000 --namespace monitoring\n",
    "\n",
    "The localhosts URLs are:\n",
    "\n",
    "- KFP = http://localhost:8080\n",
    "- MLFlow = http://localhost:5000/#/\n",
    "- Pushgateway = http://localhost:9091/\n",
    "- Prometheus = http://localhost:8080/alerts\n",
    "- Grafana = http://localhost:5000/ (user and password are admin)\n",
    "\n",
    "A sign that everything went fine with the pipeline is that the experiment 'demo-aif-run' found in runs is green in KFP, the metrics results of the notebook 'demo-aif-notebook' shows numbers in all of the columns in MLFlow, pushgateway has a job named metrics with scores for all the cases seen in the code, Prometheus alerts are working, and grafana dashboard shows different numbers. Demonstration-wise, you have reached the end, but we will still review Scaphandre metrics and demonstration debugging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc36d9",
   "metadata": {},
   "source": [
    "# Scaphandre metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b7aed",
   "metadata": {},
   "source": [
    "The relevant energy consumption metrics in Prometheus and Grafana queries, as described in https://hubblo-org.github.io/scaphandre-documentation/references/exporter-prometheus.html, are:\n",
    "\n",
    "- scaph_host_power_microwatts\n",
    "- scaph_process_power_consumption_microwatts (This shows all cluster processes)\n",
    "- scaph_host_energy_microjoule\n",
    "- scaph_socket_power_microwatts\n",
    "\n",
    "We can filter these metrics by giving suitable labels inside {} and using regex. For example, if we want to get the power consumption of the whole cluster in watts, we need to write the query 'sum(scaph_process_power_consumption_microwatts{app_kubernetes_io_managed_by=\"Helm\"}) / 1000000' either in Prometheus or Grafana. The label \"Helm\" is used because the same metrics are duplicated in the current configuration, while 1000000 changes microwatts into watts.\n",
    "\n",
    "If we want to get more granular, like the enegry consumption of Prometheus, we need to write the query scaph_process_power_consumption_microwatts{app_kubernetes_io_managed_by=\"Helm\", exe=\"prometheus\"} / 1000000. Similarly, we can get the KFP (except for inference steps) power consumption with the query sum(scaph_process_power_consumption_microwatts{app_kubernetes_io_managed_by=\"Helm\", cmdline =~\".*kfp.*\"}) / 1000000 by using regex.\n",
    "\n",
    "Other interesting labels like kubernetes_namespace only show default because Scaphandre isn't most likely configured to get the cluster namespaces. The instance label could provide even better granularity if the cluster IP addresses are stable. The most specific label is PID, which is, unfortunately, unique for all processes. There are other ways through the cmdline, which allows specifying KFP components (except for inference steps)  by putting landmark variables. So, for example, the power consumption of running the code for every component execpt inference can be queried with sum(scaph_process_power_consumption_microwatts{app_kubernetes_io_managed_by=\"Helm\", cmdline =~\".*landmark.*\"}) / 1000000. By giving more specifics, we only need the following filters for all components expect inference:\n",
    "\n",
    "- cmdline=~\".*(pull_data_component|python3-mpipinstall--quiet--no-warn-script-locationpandas~=1.4.2numpyfolktableskfp==1.8.22).*\"\n",
    "- cmdline=~\".*(preprocess_component|python3-mpipinstall--quiet--no-warn-script-locationpandas~=1.4.2scikit-learn~=1.0.2numpykfp==1.8.22).*\"\n",
    "- Train = cmdline=~\".*(train_component|python3-mpipinstall--quiet--no-warn-script-locationnumpypandas~=1.4.2aif360scikit-learn~=1.0.2mlflow~=1.25.0boto3~=1.21.0kfp==1.8.22).*\"\n",
    "- Evaluate = cmdline=~\".*(evaluate_component|python3-mpipinstall--quiet--no-warn-script-locationnumpymlflow~=1.25.0prometheus_clientkfp==1.8.22).*\"\n",
    "- Deploy = cmdline=~\".*(deploy_model_component|python3-mpipinstall--quiet--no-warn-script-locationkservekfp==1.8.22).*\"\n",
    "\n",
    "It is unknown why this matching technique does not work for inference services. We must first use regex negations to isolate KFP pipeline process runs. This can be done with the following filters:\n",
    "\n",
    "- app_kubernetes_io_managed_by=\"Helm\"\n",
    "- cmdline!~\".*(bin/|app/|conf/|--loglevelinfo|scaphandreprometheus--port8081).*\" \n",
    "- exe!~\".*(containerd-shim|nginx|postgres|sleep|workflow-contro|pause|minio|grafana-server|systemd-journal|manager|etcd|kube-apiserver|kube-controller|kube-scheduler|local-path-prov|mysqld|node|persistence_age).*\"\n",
    "\n",
    "Now, we can add further negations to isolate inference service related processes with the following filters:\n",
    "\n",
    "- app_kubernetes_io_managed_by=\"Helm\"\n",
    "- cmdline!~\".*(bin/|app/|conf/|--loglevelinfo|scaphandreprometheus--port8081|preprocess|pull_data|train|evaluate|deploy_model|numpy|python3-mpipinstall--quiet--no-warn-script-locationkservekfp==1.8.22|metadata|msklearnserver|python3server.py).*\"\n",
    "- exe!~\".*(containerd-shim|nginx|postgres|sleep|workflow-contro|pause|minio|grafana-server|systemd-journal|manager|etcd|kube-apiserver|kube-controller|kube-scheduler|local-path-prov|mysqld|node|persistence_age).*\"\n",
    "\n",
    "Unfortunately, these filters are not agnostic. They use code-specific regex matches, which means the substring negations must be updated for modified code. This problem can be fixed by properly configuring Scaphander into the cluster because it is currently in an awkward place due to requiring manual actions and not being able to get more relevant label metadata. Additionally, in the Grafana plots the sum(sum_over_time({}[1d])) / (1000000 * 1000 * 24) only approximates the cumulated daily energy consumption per hour without thinking about possible downtime, which is why sum_over_time(avg_over_time(avg())) might be a better option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b624073",
   "metadata": {},
   "source": [
    "# Demonstration debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f69b5e",
   "metadata": {},
   "source": [
    "If the run did create some errors, the first place to check for errors is the KFP logs for the components, which can be found by going into the runs, clicking the latest experiment, clicking the component with a red error, and finding its log tab. Usually, the error is caused by a coding error, so after fixing it, rerun the modified components and start the experiment.\n",
    "\n",
    "If this doesn't solve the issue, use the tests for the cluster to check if the parts are green. Notice that the cluster is fine as long as the component test gives cluster ready and the passed amount of tests is either 36 or 37. The latter case can be caused by the website storing the test data being down. There might also be other errors, but as long as KFP is capable of running, these can be ignored. The tests are: \n",
    "\n",
    "- python tests/wait_deployment_ready.py --timeout 30 (virtual enviroment recommended and OSS root directory)\n",
    "- pytest (virtual enviroment recommended)\n",
    "\n",
    "If not, removing the cluster and reinstalling everything is usually easier. A more surgical approach is to use kubectl to check the logs of pods in the given namespace and configure used YAMLs to fix the issue. When the modified YAMLs have been saved, just apply them and then rollout restart the pods. It is recommended to stop any dashboards before doing this. The required commands cluster removal, and kubectl fixing are:\n",
    "\n",
    "Cluster removal:\n",
    "- Cluster deletion = kind delete cluster --name kind-ep\n",
    "- Registry deletion = docker rm -f $(docker ps -aqf \"name=kind-registry\")\n",
    "\n",
    "Optional docker clean up:\n",
    "- Show docker configuration = docker info\n",
    "- Show containers = docker ps\n",
    "- Show all containers = docker ps -a\n",
    "- Delete containers = docker system prune (Be specific if you have other containers)\n",
    "- Show all images = docker images -a\n",
    "- Remove all images = docker image prune -a (Be specific if you have other images)\n",
    "\n",
    "Kubectl fixing\n",
    "- Show pods of monitoring namespace = kubectl get pods -n monitoring\n",
    "- Show deployment of monitoring namespace kubectl get deployment -n monitoring\n",
    "- Apply YAMLs for monitoring = kubectl apply -k deployment/monitoring (OSS root directory)\n",
    "- Show logs of a pod in monitoring namespace = kubectl logs (pod ID) -n monitoring\n",
    "- Restart prometheus of monitoring namespace = kubectl rollout restart deployment prometheus-deployment -n monitoring\n",
    "\n",
    "As a final note, it is highly recommended that the Docker root directory (Docker Root Dir in docker info) is located in a place that does not take memory from critical programs like the operating system. Docker and KFP produce a lot of data, which in the worst case take so much memory that the OS cannot start up normally without the help of IT support. To lessen the impact of these mishaps, it's always good practice to have updated backups that work in GitHub or any other cloud service of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7882f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
