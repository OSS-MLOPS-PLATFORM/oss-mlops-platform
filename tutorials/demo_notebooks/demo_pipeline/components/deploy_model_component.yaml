name: Deploy model
description: Deploy the model as a inference service with Kserve.
inputs:
- {name: model_name, type: String}
- {name: storage_uri, type: String}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kserve' 'kfp==1.8.22' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def deploy_model(model_name: str, storage_uri: str):
          """
          Deploy the model as a inference service with Kserve.
          """
          import logging
          from kubernetes import client
          from kserve import KServeClient
          from kserve import constants
          from kserve import utils
          from kserve import V1beta1InferenceService
          from kserve import V1beta1InferenceServiceSpec
          from kserve import V1beta1PredictorSpec
          from kserve import V1beta1SKLearnSpec

          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          model_uri = f"{storage_uri}/{model_name}"
          logger.info(f"MODEL URI: {model_uri}")

          # namespace = 'kserve-inference'
          namespace = utils.get_default_target_namespace()
          kserve_version='v1beta1'
          api_version = constants.KSERVE_GROUP + '/' + kserve_version


          isvc = V1beta1InferenceService(
              api_version=api_version,
              kind=constants.KSERVE_KIND,
              metadata=client.V1ObjectMeta(
                  name=model_name,
                  namespace=namespace,
                  annotations={'sidecar.istio.io/inject':'false'}
              ),
              spec=V1beta1InferenceServiceSpec(
                  predictor=V1beta1PredictorSpec(
                      service_account_name="kserve-sa",
                      sklearn=V1beta1SKLearnSpec(
                          storage_uri=model_uri
                      )
                  )
              )
          )
          KServe = KServeClient()
          KServe.create(isvc)

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - deploy_model
